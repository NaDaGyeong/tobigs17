{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2StPehwLMat"
   },
   "source": [
    "# Tobig's 17기 2주차 Optimization 과제_나다경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKIX8PqcLMaw"
   },
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6DNHHXfLMax"
   },
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EP3O4xptLMay"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oByQ9wXHLMay"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubOR3hWGLMaz"
   },
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IySSjlizLMaz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "075EQI1bLMa0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O8Ht5u8kLMa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYmxND_xLMa2"
   },
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UI0Xy0gHLMa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD7L7RwZLMa3"
   },
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xBsUSCGGLMa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bias는 절편을 구하기 위한 변수 -> 표준화 x\n",
    "\n",
    "* experience & salary는 단위, 평균, 분산의 차이가 크게 남 -> 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m9sP3nzlLMa4"
   },
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qz7xz9dbLMa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56046353, 0.85744296, 0.77488327])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QINz-EAKLMa4"
   },
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2DsTfXuLMa5"
   },
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2y05lS6xLMa5"
   },
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i] * parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOGPEhtOLMa5"
   },
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 로짓 함수는 [0,1] 범위를 입력 받아 전체 실수 범위의 값으로 변환\n",
    "\n",
    "* 대조적으로 로지스틱 함수는 전체 실수 범위에 대한 입력 값을 가져와 [0,1] 범위의 값으로 변환\n",
    "\n",
    "* 즉, 로지스틱 함수는 로짓 함수의 역함수로 특정 샘플이 클래스 1(또는 클래스 0)에 속할 조건부 확률을 예측할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2awM57u5LMa5"
   },
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1 / (1 + np.exp(-z))    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WVaZEwrdLMa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8335619066549693"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6cXHl8bLMa6"
   },
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "**선형 회귀**의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "**로지스틱 회귀**의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. **평균을 고려하는 것은 뒤에 코드에서 수행합니다)**\n",
    "## $l(p) = - \\Sigma(y_{i}logp(X_{i}) + (1-y_i)log(1-logp(X_{i})))$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 목적 함수는 보통 어떤 함수의 최댓값 또는 최솟값을 구하는 함수인데, 회귀에서 목적 함수는 평균 제곱 오차를 최소화하는 것\n",
    "\n",
    "* 평균 제곱 오차를 최소로 만들 때 사용되는 기법이 경사하강법\n",
    "\n",
    "* 그러나 로지스틱 회귀에서 평균 제곱 오차를 비용 함수로 사용하면, 경사 하강법을 사용하였을 때 찾고자 하는 최소값이 아닌 잘못된 최소값에 빠질 가능성이 매우 높음. \n",
    "\n",
    "* 이를 전체 함수에 걸쳐 최소값인 **글로벌 미니멈(Global Minimum)** 이 아닌 특정 구역에서의 최소값인 **로컬 미니멈(Local Minimum)** 에 도달했다고 함. \n",
    "\n",
    "* 로컬 미니멈에 지나치게 쉽게 빠지는 비용 함수는 cost가 가능한한 최소가 되는 가중치를 찾는다는 목적에는 좋지 않은 선택. -> 로지스틱 회귀에서의 평균 제곱 오차는 바로 그 좋지 않은 선택에 해당함.\n",
    "\n",
    "* 로지스틱 회귀에서 찾아낸 비용 함수를 크로스 엔트로피 함수라고 함. 결론적으로 로지스틱 회귀는 비용 함수로 크로스 엔트로피 함수를 사용하며, 가중치를 찾기 위해서 크로스 엔트로피 함수의 평균을 취한 함수를 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FnGRAur3LMa6"
   },
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = (y*np.log(p) + (1-y)*np.log(1-p))\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C922eXYyLMa6"
   },
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = np.dot(X, parameters.T)\n",
    "    loss = (1/2) * ((y - y_hat)**2) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0j-MhGkyLMa6"
   },
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = loss / n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uSkPS5olLMa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.374343666383798"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7612555310705833"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, mse_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위 함수들은 각 데이터 셋에 대한 목적 함수, 시그마의 안쪽 부분을 의미하고 batch_loss를 통해 시그마를 시행 -> 전체의 목적 함수의 평균을 구함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACLi9vCyLMa7"
   },
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caMA-f00LMa7"
   },
   "source": [
    "## ${\\partial\\over{\\partial \\theta_{j}}}l(\\theta)= - \\Sigma(y_{i} -\\theta^{T}X_{i})X_{ij} $ \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= - \\Sigma(y_{i} -p_{i})X_{ij}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하나의 데이터셋에 대한 목적 함수의 Gradient를 구하는 함수로 추후에 하나의 배치 데이터에 대한 전체의 Gradient를 구하고 구해진 Gradient를 바탕으로 parameter를 수정하는 과정을 거침\n",
    "\n",
    "* 따라서 위 함수는 제일 처음으로 시행되는 Gradient를 구하는 함수로 볼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "apZ0Miz5LMa7"
   },
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = np.dot(X, parameters.T)\n",
    "        gradient = (y - y_hat) * X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = (y - p) * X[j]\n",
    "    return -gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XXBe6q8gLMa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10165064675438011"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21877890548887627"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTfzKh_nLMa7"
   },
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Qby2_X1vLMa7"
   },
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rHxBS5RnLMa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.201202510032005, 20.449045931648207, 50.64912371963097]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.0695298412472, 196.8178445254023, 225.7586607953571]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'linear')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQnlDboALMa8"
   },
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LgnfT6eHLMa8"
   },
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       " array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n",
       " array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n",
       " array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69]),\n",
       " array([70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n",
       " array([80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109]),\n",
       " array([110, 111, 112, 113, 114, 115, 116, 117, 118, 119]),\n",
       " array([120, 121, 122, 123, 124, 125, 126, 127, 128, 129]),\n",
       " array([130, 131, 132, 133, 134, 135, 136, 137, 138, 139]),\n",
       " array([140, 141, 142, 143, 144, 145, 146, 147, 148, 149])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx(X_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S9fk1UTLMa8"
   },
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명: batch_size는 한 번 학습할 때 활용되는 데이터 크기를 말함. batch_size가 1이면 한 번 학습할 때 전체 데이터 중 임의의 1개의 데이터만 학습함. 현재 X_train의 크기는 150이기에 batch_size가 150이면 1개의 데이터 셋이 만들어지고, batch_size가 10이면 15개의 데이터셋 크기가 만들어 진다. 즉, batch_size가 k이면 전체 데이터의 수 / k개 만큼의 데이터셋이 만들어지고 한 번 학습할 때 k개의 데이터가 활용된다. batch_size와 데이터셋 크기는 trade-off 관계에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pMuZbkQLMa8"
   },
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient descent, 즉 경사를 따라가면서 cost를 최소화하는 단계에서 경사를 얼만큼씩 이동할건지 정해주는 것은 learning rate임. learning rate의 값을 정하는 것은 매우 중요함.\n",
    "\n",
    "* learning rate가 매우 컸을 때, **overshooting**발생 가능함. learning rate가 너무 크면, 최솟값을 찾지 못하고, 왔다갔다 하고, 심지어 cost가 증가하는, 그래프에서 튕겨져 나가는 듯한 상황이 올 수 있음.\n",
    "\n",
    "* **즉, cost를 최소화하는 함수에 있어서 cost가 줄어들지 않고 오히려 점점 늘어나는 현상이 있다면 learning rate를 의심해야 함.**\n",
    "\n",
    "* learning rate가 매우 작을때는 cost의 변화가 매우 작아 그 최솟값을 찾는데 시간이 너무 오래 걸리는 등의 현상이 발생할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "loeL51rPLMa8"
   },
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate/n\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NLB2dUVTLMa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5576589 , 0.84432177, 0.7598327 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위 함수는 구해진 Gradient를 바탕으로 설정한 학습률로 Gradient를 조정하고 그를 바탕으로 parameter를 수정하는 함수임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX8RJFd_LMa9"
   },
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 현재 반복 횟수 \n",
    "- num_epoch: 총 반복 횟수\n",
    "\n",
    "<br>\n",
    "\n",
    "BGD: 배치 경사 하강법으로 **모든 데이터**에 대하여 각각 Loss function을 계산하고, 그들의 기댓값을 오차로 하고, 그 오차를 최소화시키는 방향으로 weight를 업데이트 함. 초기 W 값에 매우 큰 영향을 받음. 연산에 대한 cost가 매우 커짐. \n",
    "\n",
    "SGD: 확률적 경사 하강법으로 **임의의 1개의 데이터**만 고려해 Loss function을 계산하고, 그 함수를 최소화하는 방향으로 weight를 업데이트 함. BGD보다는 global minimum으로 갈 가능성이 적으나 1개의 데이터를 이용해 gradient를 계산하기 때문에 Iteration이 매우 커지게 됨. 수렴하지 못하고 이상한 곳에서 왔다갔다 하는 문제가 발생할 수 있고, 수렴 속도가 매우 느림.\n",
    "\n",
    "MGD: 미니배치 경사하강법으로 **미니 배치**를 만들어서 데이터셋 일부에 대해서만 BGD 방식으로 weight를 갱신함. \n",
    "\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요\n",
    "\n",
    "batch_size=1 -> SGD \n",
    "\n",
    "batch_size=k -> MGD \n",
    "\n",
    "batch_size=whole -> BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZGbnVHbbLMa9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    \n",
    "    # train data의 독립변수의 수만큼 랜덤한 파라미터를 구함.\n",
    "    parameters = np.random.rand(N)\n",
    "    \n",
    "    # 로지스틱 회귀라면 목적함수로 minus_log_cross_entropy_i를 사용하고 선형이라면 mse_i를 사용함.\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    \n",
    "    # 초기 손실함수를 아주 큰 값으로 설정함.\n",
    "    loss = 999\n",
    "    \n",
    "    # 배치 사이즈에 맞는 데이터 셋을 구함.\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    # num_epoch만큼 학습 시행.\n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        # 한 번 학습 때 사용할 데이터 셋을 구함.\n",
    "        for idx in batch_idx_list:\n",
    "            # 한 번 학습 때 사용하는 x, y 데이터\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            # 현재 학습에 활용되는 데이터의 gradient를 구함.\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            # 구한 gradient를 바탕으로 parameter 수정.\n",
    "            parameters = step(parameters, gradients, learning_rate, len(X_batch))\n",
    "            # 수정된 parameter를 바탕으로 새로운 손실함수 구함.\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, len(X_batch))\n",
    "            \n",
    "            # 중단 조건\n",
    "            # 새로운 손실 함수와 기존 손실 함수의 차이가 tolerance보다 작다면 학습을 중단\n",
    "            # 왜냐하면 현재 데이터에 대하여 이미 충분히 학습을 완료했기에 더 이상의 학습이 필요없음.\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            # 아니라면 최신의 제일 작은 손실함수로 변경\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CTtc3eiLMa9"
   },
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnUpYC7_LMa9"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 배치 사이즈를 키우는 건 러닝 레이트를 줄이는 것과 동일한 효과를 나타냄.\n",
    "\n",
    "* 하지만 배치 사이즈의 증가는 메모리가 허락하는 한 학습의 속도를 증가시키기 때문에 train data size의 감소보다 효율적임.\n",
    "\n",
    "* learning rate를 고정한 후 BGD, SGD, MGD 방식으로 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-LS6o3aeLMa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.8430740945127624  params: [0.56502411 0.46503346 0.09835313]  gradients: [0.03558450005178442, -7.717233058364212e-06, 0.019387320715102165]\n",
      "epoch: 100  loss: 0.44236871346628903  params: [-0.81639022  0.96375404 -0.88269942]  gradients: [0.004159974052829117, -0.005814112821106385, 0.005991011383678516]\n",
      "epoch: 200  loss: 0.38660754168047023  params: [-1.04515392  1.46372886 -1.37861564]  gradients: [0.0013511584565926431, -0.004269041086211396, 0.004169264236624506]\n",
      "epoch: 300  loss: 0.3585191155604613  params: [-1.15127022  1.8348712  -1.73968253]  gradients: [0.0008788691751050966, -0.0032421938941984167, 0.0031428725662663855]\n",
      "epoch: 400  loss: 0.3417763143406986  params: [-1.22945589  2.12317513 -2.01831729]  gradients: [0.0007045981517073811, -0.002574703308299415, 0.002480804816910228]\n",
      "epoch: 500  loss: 0.3309048218938721  params: [-1.29403975  2.35602454 -2.24209264]  gradients: [0.0005941091321471083, -0.0021128888314612504, 0.0020253305713062446]\n",
      "epoch: 600  loss: 0.3234241845494949  params: [-1.34904798  2.54947228 -2.42711569]  gradients: [0.0005102514813590854, -0.0017757853079693154, 0.0016947725686594177]\n",
      "epoch: 700  loss: 0.3180563042740443  params: [-1.3965743   2.71355221 -2.58341623]  gradients: [0.00044331197351552123, -0.001519280644281376, 0.0014445833240274814]\n",
      "epoch: 800  loss: 0.3140807466411054  params: [-1.43805972  2.85492116 -2.71761731]  gradients: [0.00038872266475926357, -0.0013177379075078894, 0.0012489429309150875]\n",
      "epoch: 900  loss: 0.3110630161947802  params: [-1.4745815   2.97821675 -2.83431182]  gradients: [0.00034353659613428736, -0.0011553385291565381, 0.0010919789558194739]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.50666271,  3.08577808, -2.93585042])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, batch_size = X_train.shape[0])\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BGD 방식은 한 번 학습할 때 전체 데이터를 사용하기 때문에 GPU를 통한 병렬처리가 가능해 속도가 빨라질 수 있지만 지역 최적화에 빠질 수 있고 비용이 많이 듦.\n",
    "\n",
    "* 결과 향상을 위해서는 num_epoch를 증가시키거나 learning_rate 조정 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "x0H5tnauLMa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.27647779248511634  params: [-0.88423834  1.07430196 -1.20041478]  gradients: [0.024984273465832027, 0.013596465516766057, 0.01757870350089843]\n",
      "epoch: 100  loss: 0.0773667215652323  params: [-1.9303256   4.17501927 -4.06769092]  gradients: [0.0075385440651282555, 0.004102482890619623, 0.005304049810797012]\n",
      "epoch: 200  loss: 0.07736266552219787  params: [-1.9303681   4.1751431  -4.06780374]  gradients: [0.00753815952186717, 0.004102273621809138, 0.005303779249718695]\n",
      "epoch: 300  loss: 0.07736266518361727  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.0075381594897671435, 0.004102273604340272, 0.005303779227133413]\n",
      "epoch: 400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9303681 ,  4.17514311, -4.06780375])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 방식은 한 번 학습할 때 하나의 데이터를 활용하기 때문에 GPU를 통한 병렬처리가 불가능해 속도가 느려질 수 있지만 지역 최적화에 빠질 우려가 BGD보다는 적음.\n",
    "\n",
    "* 손실 함수가 가장 작음.\n",
    "\n",
    "* 결과 향상을 위해서는 num_epoch를 증가시키거나 learning_rate 조정 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iGfXGoJaLMa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.8537777109226673  params: [ 0.62305678  0.37251276 -0.0296035 ]  gradients: [0.03380072665317999, 0.008328560061407813, 0.03139732986066195]\n",
      "epoch: 100  loss: 0.32699946723185325  params: [-1.29372963  2.35847955 -2.24775995]  gradients: [-0.00030561031639523276, 0.0007956230596434618, 0.006697762776874057]\n",
      "epoch: 200  loss: 0.2963973702379448  params: [-1.50682427  3.08740893 -2.94033131]  gradients: [0.0011923231899555987, 0.001595043977370259, 0.004573963695356216]\n",
      "epoch: 300  loss: 0.28628099973975313  params: [-1.62550619  3.47946555 -3.30822284]  gradients: [0.0019752943655709783, 0.0019217929853666824, 0.0036936235877184816]\n",
      "epoch: 400  loss: 0.28172586052425347  params: [-1.69953243  3.72140221 -3.53370338]  gradients: [0.0024429088938002458, 0.0020960376420483067, 0.003219451407521687]\n",
      "epoch: 500  loss: 0.2793332685792222  params: [-1.7485018   3.88064362 -3.68149436]  gradients: [0.0027428793483987935, 0.0022008500510166673, 0.002932007202425716]\n",
      "epoch: 600  loss: 0.27795182274149266  params: [-1.78201919  3.9893262  -3.7820878 ]  gradients: [0.0029437481413702644, 0.0022682543243344325, 0.002746030775839736]\n",
      "epoch: 700  loss: 0.27710130311404946  params: [-1.80545494  4.06518245 -3.85216906]  gradients: [0.0030820110729545044, 0.0023134206634227304, 0.0026208289709549436]\n",
      "epoch: 800  loss: 0.2765531221044196  params: [-1.82207276  4.11890708 -3.90174036]  gradients: [0.0031789488653459702, 0.0023445086866457033, 0.0025343467643103123]\n",
      "epoch: 900  loss: 0.27618778116717635  params: [-1.83396881  4.15733534 -3.93716602]  gradients: [0.0032477778815540176, 0.002366298746327218, 0.002473568557543714]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.84246846,  4.18477668, -3.96244711])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, batch_size = 30)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MGD 방식은 한 번 학습할 때 30개의 데이터를 사용하기 때문에 GPU를 통한 병렬처리가 가능해 SGD보다는 빠르고 BGD보다는 속도가 느린 방식임.\n",
    "\n",
    "* 하지만 BGD보다 지역 최적화에 빠질 우려가 적고 2번째로 작은 손실함수를 보여줌.\n",
    "\n",
    "* 결과 향상을 위해서는 num_epoch를 증가시키거나 learning_rate 조정 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0oCaZ0tLMa-"
   },
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "syJE3oiNLMa-"
   },
   "outputs": [],
   "source": [
    "# new_param_bgd를 활용한 에측\n",
    "y_predict_bgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict_bgd.append(1)\n",
    "    else :\n",
    "        y_predict_bgd.append(0)\n",
    "\n",
    "# 초기 설정된 random_parameters를 활용한 예측        \n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)\n",
    "\n",
    "# 추가\n",
    "# new_param_sgd를 활용한 예측\n",
    "y_predict_sgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_sgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict_sgd.append(1)\n",
    "    else :\n",
    "        y_predict_sgd.append(0)\n",
    "        \n",
    "# new_param_mgd를 활용한 예측\n",
    "y_predict_mgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_mgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict_mgd.append(1)\n",
    "    else :\n",
    "        y_predict_mgd.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 임계치 0.5를 기준으로 로지스틱 함수로 예측한 확률 값을 0과 1로 구분한 코드임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZKpFItfLMa-"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "W4E1PgX5LMa-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-veTwxu4LMa-"
   },
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, y_predict, title):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "    print(f'{title}의 Confusion Matrix')\n",
    "    confusion_matrix(y_test, y_predict)\n",
    "    accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    f1 = 2 * (precision*recall) / (precision + recall)\n",
    "    print(f'{title}의 accuracy : {accuracy:.5f}, precision : {precision:.5f}, recall : {recall:.5f}, f1 : {f1:.5f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "h4_dW9rDLMa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict_bgd의 Confusion Matrix\n",
      "y_predict_bgd의 accuracy : 0.88000, precision : 0.75000, recall : 0.60000, f1 : 0.66667\n",
      "\n",
      "y_predict_sgd의 Confusion Matrix\n",
      "y_predict_sgd의 accuracy : 0.94000, precision : 0.81818, recall : 0.90000, f1 : 0.85714\n",
      "\n",
      "y_predict_mgd의 Confusion Matrix\n",
      "y_predict_mgd의 accuracy : 0.94000, precision : 0.81818, recall : 0.90000, f1 : 0.85714\n",
      "\n",
      "y_predict_random의 Confusion Matrix\n",
      "y_predict_random의 accuracy : 0.44000, precision : 0.25000, recall : 0.90000, f1 : 0.39130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, y_predict_bgd, 'y_predict_bgd')\n",
    "get_clf_eval(y_test, y_predict_sgd, 'y_predict_sgd')\n",
    "get_clf_eval(y_test, y_predict_mgd, 'y_predict_mgd')\n",
    "get_clf_eval(y_test, y_predict_random, 'y_predict_random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터의 개수:  50\n",
      "0의 개수          :  40\n",
      "1의 개수          :  10\n"
     ]
    }
   ],
   "source": [
    "print('전체 데이터의 개수: ', y_test.count())\n",
    "print(\"0의 개수          : \",sum(y_test == 0))\n",
    "print(\"1의 개수          : \",sum(y_test == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임계치 0.5를 기준으로 예측한 값에 대한 Confusion Matrix, accuracy, precision, recall, f1 score를 구한 코드임. test의 데이터 라벨이 8:2의 비율로 0에 치우져진 불균형한 상태임. 따라서 모든 값을 0으로 예측해도 정확도가 높아지기 때문에 정확도는 평가하기에 좋은 지표라고 볼 수는 없음.\n",
    "\n",
    "SGD, MGD 방식으로 학습한 데이터의 성능이 비교적 좋게 나왔음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIgqa85aLMa_"
   },
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYeIg9QNLMa_"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nv8-yhszLMa_"
   },
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "07XtxLGWLMa_"
   },
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oENC02TLMa_"
   },
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fu578YrKLMa_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79593572, 2.20593858])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "M74iqj4WLMa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.31424336894729005  params: [1.28441686 0.70404877]  gradients: [0.0687318459465774, 0.036888050574125454]\n",
      "epoch: 100  loss: 0.5271943673145046  params: [0.75067703 2.09588071]  gradients: [0.0791207446762681, 0.0522587982304327]\n",
      "epoch: 200  loss: 0.5273412649042827  params: [0.74998289 2.09720209]  gradients: [0.07910930947509084, 0.05226382454738848]\n",
      "epoch: 300  loss: 0.527341413184427  params: [0.74998219 2.09720342]  gradients: [0.07910929793793818, 0.052263829618517985]\n",
      "epoch: 400  loss: 0.5273414133340294  params: [0.74998219 2.09720342]  gradients: [0.07910929792629816, 0.05226382962363433]\n",
      "epoch: 500  loss: 0.5273414133341799  params: [0.74998219 2.09720342]  gradients: [0.07910929792628643, 0.052263829623639485]\n",
      "epoch: 600  loss: 0.5273414133341803  params: [0.74998219 2.09720342]  gradients: [0.07910929792628639, 0.0522638296236395]\n",
      "epoch: 700  loss: 0.5273414133341803  params: [0.74998219 2.09720342]  gradients: [0.07910929792628639, 0.0522638296236395]\n",
      "epoch: 800  loss: 0.5273414133341803  params: [0.74998219 2.09720342]  gradients: [0.07910929792628639, 0.0522638296236395]\n",
      "epoch: 900  loss: 0.5273414133341803  params: [0.74998219 2.09720342]  gradients: [0.07910929792628639, 0.0522638296236395]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.74998219, 2.09720342])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사하강법\n",
    "new_param = gradient_descent(X, y, model='linear')\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Ii3zBOwSLMa_"
   },
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCVynFSPLMbA"
   },
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "UoEACrbYLMbA"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijgIcAdGLMbA"
   },
   "source": [
    "정규방정식으로 구한 회귀선과 경사하강법으로 구한 회귀선 모두 비슷한 형태를 보이기 때문에 대체적으로 값을 잘 예측함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "wk3_optimization_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
